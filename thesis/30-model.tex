\chapter{Теоретическая часть}
\section{Модель}

  Для реализации была выбрана сверточная нейронная сеть из статьи \citep{collobert2011natural}.
  Из модели были удалены условные случайные поля для более быстрого обучения и проведения экспериментов.

  В качестве признаков выступают вектора слов, позиция относительно слова в предложении
  для которого предсказывается тег, капитализация и присутствие слова в газетире,
  который включен в соревнование CoNLL 2003.

  Общий алгоритм работы следующий:
  \begin{enumerate}
    \item В начало и конец предложения добавляют по одному специальному токену, чтобы его длина была
    не меньше трех. Каждый токен отображается в набор идентификаторов признаков.
    На вход нейросети поступает набор идентификаторов признаков для всего предложения.

    \item Набор идентификаторов пропускается через специальный слой (lookup table), который отображает
    каждый идентификатор в вектор весов. На выходе каждому токену предложения соответствует
    вектор.
    \item Полученные вектора объединяются в матрицу признаков предложения.
    \item Полученная матрица признаков подается на следующий
    слой, который проходится окном размера 3 и выполняет операцию свертки (temporal convolution).
    Т.е. три столбца матрицы признаков конкатенируются в один вектор и перемножаются
    на матрицу весов справа.
    На выходе получается матрица с количеством строк фиксированной длины (для любого предложения).
    \item Затем извлекается максимум по каждой строке (max over time).
    Таким образом все предложения любой длины получают вектор признаков фиксированной длины.
    \item Полученный вектор признаков подается на полносвязный слой.
    \item Затем выход полносвязного слоя подается на выходной слой, который возвращает вероятность
    для каждого тега (softmax).
  \end{enumerate}

  В качестве функции потерь используется кросс-энтропия (cross entropy).

  Подробная математическая модель описана в статье \citep{collobert2011natural}.

  \subsection{Синтактико-семантические признаки}
    Существует много инструментов для получения дополнительных признаков для слова.
    Для извлечения синтаксических признаков часто используют MaltParser \citep{nivre2006maltparser}.
    Для получения семантических признаков применяют BabelNet \citep{navigli2010babelnet}.

    В данной работе для получения синтактико-семантических признаков используется Compreno.
    Вершины синтактико-семантического дерева Compreno кодировались бинарным представлением
    и соотносились с токенами исходного
    текста\footnote{Почти для всех токенов в соответствующем дереве нашлась соответствующая вершина.
    Токены для которых не была найдена вершина, кодировались специальным признаком 83951},
    тем самым наделяя их синтактико-се\-ман\-ти\-ческими признаками.
    Размерность пространства синтактико-се\-ман\-ти\-ческих признаков получилась равной 83950.

    Плотные вектора большой размерности сильно замедляют процесс оптимизации и для хорошего
    обучения требуется много данных и вычислительных ресурсов.
    В таких случаях часто применяют методы для уменьшения размерности,
    например сингулярное разложение или автоэнкодеры. Минусом таких методов является потеря информации
    после сжатия.

    Если же вектора большой размерности разреженные, то используют специальные методы для
    работы с такими данными \citep{davissurvey}.

    В данной работе предлагается 2 способа внедрения синтактико-семантических признаков:
    \begin{itemize}
      \item сжать синтактико-семантические вектора с помощью сингулярного разложения (SVD) и добавить
      как еще один Lookup Table в сверточную нейронную сеть;
      \item добавить еще одну нейронную сеть для синтактико-семантических признаков и оптимизировать
      её вместе со сверточной нейронной сетью.
    \end{itemize}
